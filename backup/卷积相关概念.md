​
**卷积计算层**
深度Depth=kernel的数量=filter的数量
步长Stride：窗口一次滑动的长度
填充值padding：当步长设置大时，最后剩下的图像边缘不够滑动，在整个图像数个像素的宽度
输出窗口宽=（输入宽口长/宽 - 卷积核长/宽 + 2 * padding大小）/ stride + 1

输出窗口高 = 输出窗口宽

**池化**
作用：压缩图像尺寸，同时保证图像的质量，分层压缩。

方法：取区域平均/最大

常用**优化**方法：SGD
全连接层FC
作用：作为分类器，将特征映射到样本标记空间。

**常用的Net:**
LeNet，这是最早用于数字识别的CNN
AlexNet， 2012 ILSVRC比赛远超第2名的CNN，比
LeNet更深，用多层小卷积层叠加替换单大卷积层。
ZF Net， 2013 ILSVRC比赛冠军
GoogLeNet， 2014 ILSVRC比赛冠军
VGGNet， 2014 ILSVRC比赛中的模型，图像识别略差于GoogLeNet，但是在很多图像转化学习问题(比如object detection)上效果奇好
**激活函数**
一般是Relu，特殊会用leaky Relu,tanh

sigmoid作为指数函数，在层数较多时，某些值会积累，并指数级增长，导致梯度爆炸

**fine-tuning**
作用：将其他模型训练好的权重，直接拿过来做初始化用，比慢慢训练要快很多。

**下采样**
作用：把图像缩小

方法：区域像素点取平均

**上采样**
作用：把图像变大

方法：插值

pytorch.nn.Conv2d(in_channel,out_channel,kernel_size,stride)
如nn.Conv2d(16, 33, 3, 2, 2,1)，代表in_channel=16,out_channel=33,kernel_size=(3,2),stride=(2,1)

​